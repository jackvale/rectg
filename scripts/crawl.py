#!/usr/bin/env python3
"""
Telegram å…¬å¼€é¡µé¢çˆ¬è™«
ä» SQLite çš„ links è¡¨ä¸­è·å–é“¾æ¥ï¼Œçˆ¬å–å…¬å¼€ä¿¡æ¯ï¼Œç»“æœå­˜å…¥ entries è¡¨ã€‚

å‰ç½®æ¡ä»¶:
    å…ˆè¿è¡Œ parse_links.py å°† README ä¸­çš„é“¾æ¥å¯¼å…¥ links è¡¨ã€‚

ç”¨æ³•:
    python3 scripts/crawl.py              # çˆ¬å–å…¨éƒ¨ï¼ˆæ–­ç‚¹ç»­çˆ¬ï¼‰
    python3 scripts/crawl.py --limit 10   # åªçˆ¬å‰ 10 ä¸ªï¼ˆæµ‹è¯•ç”¨ï¼‰
    python3 scripts/crawl.py --new        # åªçˆ¬æ–°é“¾æ¥
    python3 scripts/crawl.py --no-resume  # æ¸…ç©º entries è¡¨ï¼Œä»å¤´å¼€å§‹
"""
from __future__ import annotations

import argparse
import base64
import json
import logging
import random
import re
import sqlite3
import sys
import time
from datetime import datetime
from pathlib import Path

import requests
import opencc
from bs4 import BeautifulSoup

# ---------------------------------------------------------------------------
# å¸¸é‡
# ---------------------------------------------------------------------------

ROOT_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = ROOT_DIR / "data"
DB_PATH = DATA_DIR / "rectg.db"
LOG_PATH = DATA_DIR / "crawl.log"

# é™æµé…ç½®ï¼ˆä¿å®ˆï¼‰
MIN_DELAY = 3          # æ¯æ¬¡è¯·æ±‚æœ€å°é—´éš”ï¼ˆç§’ï¼‰
MAX_DELAY = 6          # æ¯æ¬¡è¯·æ±‚æœ€å¤§é—´éš”ï¼ˆç§’ï¼‰
BATCH_SIZE = 50        # æ¯æ‰¹æ¬¡æ¡æ•°
BATCH_PAUSE = 60       # æ‰¹æ¬¡é—´æš‚åœï¼ˆç§’ï¼‰
RETRY_BASE = 60        # 429 é€€é¿åŸºç¡€ç­‰å¾…ï¼ˆç§’ï¼‰
RETRY_MAX = 300        # 429 æœ€å¤§ç­‰å¾…ï¼ˆç§’ï¼‰
MAX_RETRIES = 3        # æœ€å¤§é‡è¯•æ¬¡æ•°

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
}

# è¿‡æ»¤é˜ˆå€¼
MIN_CHANNEL_SUBSCRIBERS = 1000
MIN_GROUP_MEMBERS = 200
INACTIVE_DAYS_THRESHOLD = 90

# ä¸­æ–‡å­—ç¬¦ Unicode èŒƒå›´
_CJK_RANGES = re.compile(r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]')


# ---------------------------------------------------------------------------
# æ—¥å¿—é…ç½®
# ---------------------------------------------------------------------------

def setup_logging(log_path: Path) -> logging.Logger:
    """é…ç½®æ—¥å¿—ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ã€‚"""
    log_path.parent.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger("crawl")
    logger.setLevel(logging.DEBUG)

    # æ§åˆ¶å°ï¼šç®€æ´æ ¼å¼
    console = logging.StreamHandler(sys.stdout)
    console.setLevel(logging.INFO)
    console.setFormatter(logging.Formatter("%(message)s"))
    # å¼ºåˆ¶ flush
    console.stream = type('FlushStream', (), {
        'write': lambda self, msg: (sys.stdout.write(msg), sys.stdout.flush()),
        'flush': lambda self: sys.stdout.flush(),
    })()
    logger.addHandler(console)

    # æ–‡ä»¶ï¼šè¯¦ç»†æ ¼å¼ï¼Œå¸¦æ—¶é—´æˆ³
    file_handler = logging.FileHandler(str(log_path), encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    ))
    logger.addHandler(file_handler)

    return logger


log = logging.getLogger("crawl")


# ---------------------------------------------------------------------------
# è¿›åº¦è¿½è¸ªå™¨
# ---------------------------------------------------------------------------

class ProgressTracker:
    """è¿½è¸ªçˆ¬å–è¿›åº¦å¹¶è®¡ç®— ETAã€‚"""

    def __init__(self, total: int):
        self.total = total
        self.done = 0
        self.kept = 0
        self.filtered = 0
        self.start_time = time.time()

    def tick(self, keep: bool):
        self.done += 1
        if keep:
            self.kept += 1
        else:
            self.filtered += 1

    def progress_str(self) -> str:
        elapsed = time.time() - self.start_time
        pct = self.done * 100 / self.total if self.total else 0
        remaining = self.total - self.done

        if self.done > 0:
            avg_time = elapsed / self.done
            eta_seconds = remaining * avg_time
            eta_min = int(eta_seconds // 60)
            eta_sec = int(eta_seconds % 60)
            eta_str = f"{eta_min}m{eta_sec}s"
        else:
            eta_str = "è®¡ç®—ä¸­"

        elapsed_min = int(elapsed // 60)
        elapsed_sec = int(elapsed % 60)

        return (
            f"[{self.done}/{self.total}] {pct:.1f}% "
            f"| âœ…{self.kept} âŒ{self.filtered} "
            f"| è€—æ—¶ {elapsed_min}m{elapsed_sec}s "
            f"| é¢„è®¡å‰©ä½™ {eta_str}"
        )

    def summary_str(self) -> str:
        elapsed = time.time() - self.start_time
        elapsed_min = int(elapsed // 60)
        elapsed_sec = int(elapsed % 60)
        return (
            f"æ€»è®¡: {self.done} | ä¿ç•™: {self.kept} | è¿‡æ»¤: {self.filtered} "
            f"| æ€»è€—æ—¶: {elapsed_min}m{elapsed_sec}s"
        )


# ---------------------------------------------------------------------------
# SQLite æ•°æ®åº“
# ---------------------------------------------------------------------------

def init_db(db_path: Path) -> sqlite3.Connection:
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»º entries è¡¨ã€‚"""
    conn = sqlite3.connect(str(db_path))
    conn.row_factory = sqlite3.Row
    conn.execute("""
        CREATE TABLE IF NOT EXISTS entries (
            id              INTEGER PRIMARY KEY AUTOINCREMENT,
            telegram_id     INTEGER,
            username        TEXT UNIQUE,
            url             TEXT NOT NULL UNIQUE,
            type            TEXT,
            title           TEXT,
            description     TEXT,
            clean_title     TEXT,
            clean_desc      TEXT,
            category        TEXT,
            avatar          TEXT,
            count           INTEGER,
            last_active     TEXT,
            valid           INTEGER DEFAULT 0,
            private         INTEGER DEFAULT 0,
            keep            INTEGER DEFAULT 0,
            filter_reason   TEXT,
            created_at      TEXT NOT NULL,
            updated_at      TEXT NOT NULL
        )
    """)
    conn.commit()
    return conn


def upsert_entry(conn: sqlite3.Connection, data: dict):
    """æ’å…¥æˆ–æ›´æ–°ä¸€æ¡ entries è®°å½•ã€‚"""
    now = datetime.now().isoformat()

    # å…ˆæ£€æŸ¥ url æ˜¯å¦å·²å­˜åœ¨
    existing = conn.execute(
        "SELECT id, created_at FROM entries WHERE url = ?",
        (data["url"],),
    ).fetchone()

    if existing:
        data["created_at"] = existing["created_at"]
        data["updated_at"] = now
        conn.execute("""
            UPDATE entries SET
                telegram_id   = :telegram_id,
                username      = :username,
                type          = :type,
                title         = :title,
                description   = :description,
                avatar        = :avatar,
                count         = :count,
                last_active   = :last_active,
                valid         = :valid,
                private       = :private,
                keep          = :keep,
                filter_reason = :filter_reason,
                updated_at    = :updated_at
            WHERE url = :url
        """, data)
    else:
        # æ£€æŸ¥ username æ˜¯å¦å·²å­˜åœ¨ï¼ˆä¸åŒ URL æŒ‡å‘åŒä¸€é¢‘é“ï¼Œå¦‚ fakeye?boost å’Œ fakeyeï¼‰
        if data.get("username"):
            dup = conn.execute(
                "SELECT id, url FROM entries WHERE username = ?",
                (data["username"],),
            ).fetchone()
            if dup:
                log.info("       â­ï¸  è·³è¿‡: åŒä¸€é¢‘é“å·²å­˜åœ¨ (%s)", dup["url"])
                return

        data["created_at"] = now
        data["updated_at"] = now
        conn.execute("""
            INSERT INTO entries (
                telegram_id, username, url, type,
                title, description, avatar,
                count, last_active,
                valid, private, keep, filter_reason,
                created_at, updated_at
            ) VALUES (
                :telegram_id, :username, :url, :type,
                :title, :description, :avatar,
                :count, :last_active,
                :valid, :private, :keep, :filter_reason,
                :created_at, :updated_at
            )
        """, data)
    conn.commit()


# ---------------------------------------------------------------------------
# çˆ¬å–é€»è¾‘
# ---------------------------------------------------------------------------

def parse_subscriber_text(text: str) -> tuple:
    """è§£æé¡µé¢ä¸Šçš„äººæ°”æ•°æ®æ–‡æœ¬ã€‚"""
    text = text.strip()
    if not text:
        return None, None

    m = re.search(r"([\d\s\xa0]+)\s*subscribers?", text, re.IGNORECASE)
    if m:
        count = int(m.group(1).replace(" ", "").replace("\xa0", ""))
        return "channel", count

    m = re.search(r"([\d\s\xa0]+)\s*members?", text, re.IGNORECASE)
    if m:
        count = int(m.group(1).replace(" ", "").replace("\xa0", ""))
        return "group", count

    m = re.search(r"([\d\s\xa0]+)\s*monthly\s*users?", text, re.IGNORECASE)
    if m:
        count = int(m.group(1).replace(" ", "").replace("\xa0", ""))
        return "bot", count

    return None, None


def crawl_page(session: requests.Session, url: str, username: str) -> dict:
    """çˆ¬å–å•ä¸ª t.me é¡µé¢ï¼Œæå–å…¬å¼€ä¿¡æ¯ã€‚"""
    result = {
        "url": url,
        "username": username,
        "telegram_id": None,
        "valid": 0,
        "private": 0,
        "type": None,
        "title": None,
        "description": None,
        "avatar": None,
        "count": None,
        "last_active": None,
    }

    if username is None:
        result["private"] = 1
        log.debug("  è·³è¿‡: æ—  usernameï¼ˆç§æœ‰é‚€è¯·é“¾æ¥ï¼‰")
        return result

    canonical_url = f"https://t.me/{username}"
    log.debug("  GET %s", canonical_url)
    resp = _request_with_retry(session, canonical_url)
    if resp is None or resp.status_code != 200:
        log.debug("  HTTP å¤±è´¥: %s", resp.status_code if resp else "æ— å“åº”")
        return result

    soup = BeautifulSoup(resp.text, "lxml")
    result["valid"] = 1

    # æ£€æŸ¥æ˜¯å¦ç§å¯†
    page_text = soup.get_text(separator=" ", strip=True).lower()
    private_keywords = [
        "this channel is private",
        "this group is private",
        "this channel can't be displayed",
    ]
    extra_div = soup.find("div", class_="tgme_page_extra")
    if any(kw in page_text for kw in private_keywords):
        if not extra_div or not extra_div.get_text(strip=True):
            result["private"] = 1

    # æå– meta ä¿¡æ¯
    og_title = soup.find("meta", property="og:title")
    if og_title:
        title = og_title.get("content", "").strip()
        title = re.sub(r"^Telegram:\s*(Contact|View|Launch)\s*@?\s*", "", title)
        if title:
            result["title"] = title

    og_desc = soup.find("meta", property="og:description")
    if og_desc:
        result["description"] = og_desc.get("content", "").strip()

    og_image = soup.find("meta", property="og:image")
    if og_image:
        avatar_url = og_image.get("content", "").strip()
        if avatar_url and "telegram.org/img/" not in avatar_url:
            result["avatar"] = avatar_url

    # æå–äººæ°”æ•°æ®
    if extra_div:
        extra_text = extra_div.get_text(strip=True)
        detected_type, count = parse_subscriber_text(extra_text)
        if detected_type:
            result["type"] = detected_type
            result["count"] = count

    return result


def crawl_preview_page(session: requests.Session, username: str) -> dict:
    """çˆ¬å–é¢‘é“ /s/ é¢„è§ˆé¡µï¼Œè·å–æœ€åæ´»è·ƒæ—¶é—´å’Œ Telegram IDã€‚"""
    info = {"last_active": None, "telegram_id": None}
    url = f"https://t.me/s/{username}"
    log.debug("  GET %s", url)
    resp = _request_with_retry(session, url)
    if resp is None or resp.status_code != 200:
        log.debug("  /s/ é¡µé¢ä¸å¯ç”¨")
        return info

    soup = BeautifulSoup(resp.text, "lxml")

    # æå– Telegram ID
    data_view_el = soup.find(attrs={"data-view": True})
    if data_view_el:
        try:
            raw = data_view_el["data-view"]
            padding = 4 - len(raw) % 4
            if padding != 4:
                raw += "=" * padding
            decoded = base64.b64decode(raw).decode("utf-8")
            view_data = json.loads(decoded)
            if "c" in view_data:
                short_id = view_data["c"]
                info["telegram_id"] = int(f"-100{abs(short_id)}")
        except Exception as e:
            log.debug("  è§£æ telegram_id å¤±è´¥: %s", e)

    # æå–æœ€åæ´»è·ƒæ—¶é—´
    date_elements = soup.find_all(attrs={"datetime": True})
    if date_elements:
        dates = [d["datetime"] for d in date_elements]
        dates.sort()
        if dates:
            info["last_active"] = dates[-1]
    else:
        time_elements = soup.find_all("time")
        if time_elements:
            dates = [t.get("datetime") for t in time_elements if t.get("datetime")]
            if dates:
                dates.sort()
                info["last_active"] = dates[-1]

    return info


# ---------------------------------------------------------------------------
# HTTP è¯·æ±‚
# ---------------------------------------------------------------------------

def _request_with_retry(
    session: requests.Session,
    url: str,
    max_retries: int = MAX_RETRIES,
):
    """å¸¦æŒ‡æ•°é€€é¿çš„ HTTP GET è¯·æ±‚ã€‚"""
    for attempt in range(max_retries):
        try:
            resp = session.get(url, headers=HEADERS, timeout=15)
        except requests.RequestException as e:
            log.warning("  âš  è¯·æ±‚å¼‚å¸¸: %s", e)
            if attempt < max_retries - 1:
                wait = min(RETRY_BASE * (2 ** attempt), RETRY_MAX)
                log.info("  â³ ç­‰å¾… %ds åé‡è¯• (ç¬¬ %d/%d æ¬¡)...", wait, attempt + 1, max_retries)
                time.sleep(wait)
                continue
            return None

        if resp.status_code == 429:
            wait = min(RETRY_BASE * (2 ** attempt), RETRY_MAX)
            log.warning("  âš  429 Too Many Requestsï¼Œç­‰å¾… %ds... (ç¬¬ %d/%d æ¬¡)", wait, attempt + 1, max_retries)
            time.sleep(wait)
            continue

        log.debug("  HTTP %d (%d bytes)", resp.status_code, len(resp.content))
        return resp

    return None


# ---------------------------------------------------------------------------
# è¿‡æ»¤
# ---------------------------------------------------------------------------

# OpenCC ç¹â†’ç®€è½¬æ¢å™¨ï¼ˆå…¨å±€å¤ç”¨ï¼‰
_t2s_converter = opencc.OpenCC('t2s')

# ç¹ä½“ä¸­æ–‡åˆ¤å®šé˜ˆå€¼ï¼šè½¬æ¢å‰åå­—ç¬¦å·®å¼‚ç‡è¶…è¿‡æ­¤æ¯”ä¾‹è§†ä¸ºç¹ä½“
_TRADITIONAL_RATIO_THRESHOLD = 0.10


def _contains_chinese(text: str) -> bool:
    if not text:
        return False
    return bool(_CJK_RANGES.search(text))


def _is_traditional_chinese(text: str) -> bool:
    """ä½¿ç”¨ OpenCC åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºç¹ä½“ä¸­æ–‡ã€‚
    å°†æ–‡æœ¬ä»ç¹ä½“è½¬ä¸ºç®€ä½“ï¼Œæ¯”è¾ƒè½¬æ¢å‰åçš„å·®å¼‚æ¯”ä¾‹ã€‚
    """
    if not text:
        return False
    cjk_chars = _CJK_RANGES.findall(text)
    if len(cjk_chars) < 5:
        return False

    simplified = _t2s_converter.convert(text)
    # ç»Ÿè®¡è½¬æ¢å‰åä¸åŒçš„å­—ç¬¦æ•°
    diff_count = sum(1 for a, b in zip(text, simplified) if a != b)
    total = max(len(text), 1)
    ratio = diff_count / total
    return ratio >= _TRADITIONAL_RATIO_THRESHOLD


def _is_simplified_chinese_entry(entry: dict) -> bool:
    """åˆ¤æ–­æ¡ç›®æ˜¯å¦ä¸ºç®€ä½“ä¸­æ–‡å†…å®¹ï¼ˆåŒ…å«ä¸­æ–‡ä¸”ä¸æ˜¯ç¹ä½“ï¼‰ã€‚"""
    has_chinese = False
    combined_text = ""
    for field in ("title", "description"):
        text = entry.get(field) or ""
        if _contains_chinese(text):
            has_chinese = True
        combined_text += text

    if not has_chinese:
        return False

    if _is_traditional_chinese(combined_text):
        return False

    return True


def _is_inactive_channel(entry: dict) -> bool:
    last_active = entry.get("last_active")
    if not last_active:
        return False
    try:
        dt_str = last_active.replace("+00:00", "").replace("Z", "")
        last_dt = datetime.fromisoformat(dt_str)
        return (datetime.now() - last_dt).days > INACTIVE_DAYS_THRESHOLD
    except (ValueError, TypeError):
        return False


def _inactive_days(entry: dict) -> int:
    last_active = entry.get("last_active", "")
    try:
        dt_str = last_active.replace("+00:00", "").replace("Z", "")
        last_dt = datetime.fromisoformat(dt_str)
        return (datetime.now() - last_dt).days
    except (ValueError, TypeError):
        return 0


def should_keep(entry: dict) -> tuple:
    """åˆ¤æ–­æ¡ç›®æ˜¯å¦åº”è¯¥ä¿ç•™ã€‚è¿”å› (keep, reason)ã€‚"""
    if not entry.get("valid"):
        return False, "é“¾æ¥æ— æ•ˆ"
    if entry.get("private"):
        return False, "ç§å¯†é¢‘é“/ç¾¤ç»„"

    entry_type = entry.get("type")

    if entry_type is None:
        return False, "æ— æ³•è¯†åˆ«ç±»å‹"

    if not _contains_chinese((entry.get("title") or "") + (entry.get("description") or "")):
        return False, "éä¸­æ–‡å†…å®¹"

    if not _is_simplified_chinese_entry(entry):
        return False, "ç¹ä½“ä¸­æ–‡å†…å®¹"

    if entry_type == "channel":
        count = entry.get("count") or 0
        if count < MIN_CHANNEL_SUBSCRIBERS:
            return False, f"è®¢é˜…æ•°ä¸è¶³ ({count} < {MIN_CHANNEL_SUBSCRIBERS})"
        if _is_inactive_channel(entry):
            days = _inactive_days(entry)
            return False, f"é¢‘é“ä¸æ´»è·ƒ ({days}å¤©æœªæ›´æ–°)"
    elif entry_type == "group":
        count = entry.get("count") or 0
        if count < MIN_GROUP_MEMBERS:
            return False, f"æˆå‘˜æ•°ä¸è¶³ ({count} < {MIN_GROUP_MEMBERS})"
    elif entry_type == "bot":
        count = entry.get("count")
        if count is None or count == 0:
            return False, "æ— æœˆæ´»ç”¨æˆ·æ•°æ®"

    return True, ""


# ---------------------------------------------------------------------------
# ä¸»æµç¨‹
# ---------------------------------------------------------------------------

def main():
    global log

    parser = argparse.ArgumentParser(description="Telegram å…¬å¼€é¡µé¢çˆ¬è™«")
    parser.add_argument("--limit", type=int, default=0, help="é™åˆ¶çˆ¬å–æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰")
    parser.add_argument("--new", action="store_true", help="åªçˆ¬å–å°šæœªçˆ¬è¿‡çš„æ–°é“¾æ¥")
    parser.add_argument("--no-resume", action="store_true", help="æ¸…ç©º entries è¡¨ï¼Œä»å¤´å¼€å§‹")
    parser.add_argument("--no-active", action="store_true", help="è·³è¿‡ /s/ é¡µé¢çˆ¬å–")
    args = parser.parse_args()

    # åˆå§‹åŒ–æ—¥å¿—
    log = setup_logging(LOG_PATH)

    log.info("=" * 60)
    log.info("  Telegram å…¬å¼€é¡µé¢çˆ¬è™«")
    log.info("  å¯åŠ¨æ—¶é—´: %s", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    log.info("  æ—¥å¿—æ–‡ä»¶: %s", LOG_PATH)
    log.info("=" * 60)

    if not DB_PATH.exists():
        log.error("âŒ æœªæ‰¾åˆ°æ•°æ®åº“: %s", DB_PATH)
        log.error("   è¯·å…ˆè¿è¡Œ: python3 scripts/parse_links.py")
        sys.exit(1)

    conn = init_db(DB_PATH)

    # æ¸…ç©ºæ¨¡å¼
    if args.no_resume:
        conn.execute("DELETE FROM entries")
        conn.commit()
        log.info("ğŸ—‘ï¸  å·²æ¸…ç©º entries è¡¨")

    # 1. ä» links è¡¨è·å–å¾…çˆ¬é“¾æ¥
    if args.new or (not args.no_resume):
        links = conn.execute("""
            SELECT l.url, l.username, l.name, l.type_hint
            FROM links l
            LEFT JOIN entries e ON l.url = e.url
            WHERE e.url IS NULL
            ORDER BY l.id
        """).fetchall()
    else:
        links = conn.execute("""
            SELECT url, username, name, type_hint FROM links ORDER BY id
        """).fetchall()

    links = [dict(row) for row in links]

    if not links:
        if args.new:
            log.info("âœ… æ²¡æœ‰æ–°é“¾æ¥éœ€è¦çˆ¬å–")
        else:
            log.error("âŒ links è¡¨ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ: python3 scripts/parse_links.py")
        sys.exit(0)

    total_links = conn.execute("SELECT COUNT(*) FROM links").fetchone()[0]
    total_entries = conn.execute("SELECT COUNT(*) FROM entries").fetchone()[0]
    log.info("")
    log.info("ğŸ“Š æ•°æ®åº“æ¦‚è§ˆ:")
    log.info("   links è¡¨:   %d æ¡", total_links)
    log.info("   entries è¡¨: %d æ¡", total_entries)
    log.info("   å¾…çˆ¬å–:     %d æ¡", len(links))

    if args.limit > 0:
        links = links[:args.limit]
        log.info("   é™åˆ¶çˆ¬å–:   å‰ %d ä¸ª", args.limit)

    # 2. å¼€å§‹çˆ¬å–
    session = requests.Session()
    request_count = 0
    total = len(links)
    tracker = ProgressTracker(total)

    log.info("")
    log.info("ğŸ•·ï¸  å¼€å§‹çˆ¬å–...")
    log.info("-" * 60)

    for i, link in enumerate(links):
        url = link["url"]
        username = link["username"]

        log.info("")
        log.info("[%d/%d] ğŸ” %s", i + 1, total, link["name"])
        log.info("       %s", url)

        # çˆ¬å–ä¸»é¡µ
        result = crawl_page(session, url, username)
        request_count += 1

        # å¦‚æœçˆ¬å–æœªè¯†åˆ«ç±»å‹ï¼Œç”¨ links è¡¨çš„ type_hint æ¨æ–­
        if result["type"] is None and link.get("type_hint"):
            result["type"] = link["type_hint"]
            log.debug("  ç±»å‹ç”± type_hint æ¨æ–­: %s", link["type_hint"])

        # çˆ¬å– /s/ é¡µé¢ï¼ˆä»…æœ‰æ•ˆé¢‘é“ï¼‰
        if (
            not args.no_active
            and result.get("valid")
            and result.get("type") == "channel"
            and not result.get("private")
            and username
        ):
            _random_delay()
            preview = crawl_preview_page(session, username)
            result["last_active"] = preview["last_active"]
            if preview["telegram_id"]:
                result["telegram_id"] = preview["telegram_id"]
            request_count += 1

        # è¿‡æ»¤åˆ¤æ–­
        keep, reason = should_keep(result)
        result["keep"] = 1 if keep else 0
        result["filter_reason"] = reason

        # æ—¥å¿—è¾“å‡º
        if result.get("valid"):
            type_label = {"channel": "é¢‘é“", "group": "ç¾¤ç»„", "bot": "æœºå™¨äºº"}.get(result.get("type"), "æœªçŸ¥")
            count_val = result.get("count")
            count_str = f"{count_val:,}" if count_val is not None else "-"

            if keep:
                log.info("       âœ… ä¿ç•™ | %s | %s", type_label, count_str)
            else:
                log.info("       âŒ è¿‡æ»¤ | %s | %s | %s", type_label, count_str, reason)

            if result.get("telegram_id"):
                log.debug("       ğŸ†” ID: %s", result["telegram_id"])
            if result.get("last_active"):
                log.debug("       ğŸ“… æœ€åæ´»è·ƒ: %s", result["last_active"])
        else:
            log.info("       âŒ æ— æ•ˆé“¾æ¥")

        # æ›´æ–°è¿›åº¦
        tracker.tick(keep)

        # å†™å…¥æ•°æ®åº“
        upsert_entry(conn, result)

        # æ¯ 10 æ¡æ‰“å°ä¸€æ¬¡è¿›åº¦æ‘˜è¦
        if (i + 1) % 10 == 0:
            log.info("")
            log.info("ğŸ“ˆ %s", tracker.progress_str())

        # é™æµ
        _random_delay()

        # æ‰¹æ¬¡æš‚åœ
        if request_count > 0 and request_count % BATCH_SIZE == 0:
            log.info("")
            log.info("â¸ï¸  å·²çˆ¬ %d æ¬¡è¯·æ±‚ï¼Œæš‚åœ %ds...", request_count, BATCH_PAUSE)
            time.sleep(BATCH_PAUSE)
            log.info("â–¶ï¸  ç»§ç»­çˆ¬å–...")

    # 3. æ‰“å°ç»Ÿè®¡
    stats = conn.execute("""
        SELECT
            COUNT(*) as total,
            SUM(keep) as kept,
            SUM(CASE WHEN keep = 0 THEN 1 ELSE 0 END) as filtered,
            SUM(CASE WHEN type = 'channel' AND keep = 1 THEN 1 ELSE 0 END) as channels,
            SUM(CASE WHEN type = 'group' AND keep = 1 THEN 1 ELSE 0 END) as groups,
            SUM(CASE WHEN type = 'bot' AND keep = 1 THEN 1 ELSE 0 END) as bots
        FROM entries
    """).fetchone()

    filter_reasons = conn.execute("""
        SELECT filter_reason, COUNT(*) as cnt
        FROM entries WHERE keep = 0 AND filter_reason IS NOT NULL
        GROUP BY filter_reason ORDER BY cnt DESC
    """).fetchall()

    conn.close()

    log.info("")
    log.info("=" * 60)
    log.info("  ğŸ“Š çˆ¬å–å®Œæˆ")
    log.info("=" * 60)
    log.info("  %s", tracker.summary_str())
    log.info("")
    log.info("  æ•°æ®åº“ entries è¡¨:")
    log.info("    æ€»æ¡ç›®:    %s", stats['total'])
    log.info("    ä¿ç•™:      %s", stats['kept'])
    log.info("    è¿‡æ»¤:      %s", stats['filtered'])
    log.info("    â”œ é¢‘é“:    %s", stats['channels'])
    log.info("    â”œ ç¾¤ç»„:    %s", stats['groups'])
    log.info("    â”” æœºå™¨äºº:  %s", stats['bots'])

    if filter_reasons:
        log.info("")
        log.info("  è¿‡æ»¤åŸå› :")
        for row in filter_reasons:
            log.info("    âŒ %s: %s", row['filter_reason'], row['cnt'])

    log.info("")
    log.info("  æ—¥å¿—: %s", LOG_PATH)
    log.info("  æ•°æ®åº“: %s", DB_PATH)


def _random_delay():
    delay = random.uniform(MIN_DELAY, MAX_DELAY)
    time.sleep(delay)


if __name__ == "__main__":
    main()
